{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dibya-pati/text-prediction/blob/master/characterPrediction_seq50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbHKuVqxJuPk"
   },
   "source": [
    "we will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section we will use this model to generate new sequences of characters.\n",
    "\n",
    "Let’s start off by importing the classes and functions we intend to use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KeqN1p7OJtcP"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiuJD8oeK1Us"
   },
   "source": [
    "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dXUa39y6K2UX"
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 8118: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e9caa9cd8b78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load ascii text and covert to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"wonderland.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mraw_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mraw_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 8118: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqWTh9X4NAV5"
   },
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amPMvYIbNBzU"
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "colab_type": "code",
    "id": "IcDg390ZNNLM",
    "outputId": "6262dc74-118c-4a24-d3ab-6423640040eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '*': 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '0': 9,\n",
       " '3': 10,\n",
       " ':': 11,\n",
       " ';': 12,\n",
       " '?': 13,\n",
       " '[': 14,\n",
       " ']': 15,\n",
       " '_': 16,\n",
       " 'a': 17,\n",
       " 'b': 18,\n",
       " 'c': 19,\n",
       " 'd': 20,\n",
       " 'e': 21,\n",
       " 'f': 22,\n",
       " 'g': 23,\n",
       " 'h': 24,\n",
       " 'i': 25,\n",
       " 'j': 26,\n",
       " 'k': 27,\n",
       " 'l': 28,\n",
       " 'm': 29,\n",
       " 'n': 30,\n",
       " 'o': 31,\n",
       " 'p': 32,\n",
       " 'q': 33,\n",
       " 'r': 34,\n",
       " 's': 35,\n",
       " 't': 36,\n",
       " 'u': 37,\n",
       " 'v': 38,\n",
       " 'w': 39,\n",
       " 'x': 40,\n",
       " 'y': 41,\n",
       " 'z': 42,\n",
       " '‘': 43,\n",
       " '’': 44,\n",
       " '“': 45,\n",
       " '”': 46}"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PysLUUz3Ng6f"
   },
   "source": [
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ZYfPOZFwNj05",
    "outputId": "d5590315-9190-4536-997c-da63f5c265ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144422\n",
      "Total Vocab:  47\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-noVoE5mS5v"
   },
   "source": [
    "As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-sKHWn1EmT7X",
    "outputId": "19e1c623-1d94-40a9-f550-eb326967ddfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144372\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 50\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpTPcy_Umw5u"
   },
   "source": [
    "First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n",
    "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 47, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UCdKD0RGmx9O"
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mzbquHNm9Zl"
   },
   "source": [
    "We can now define our LSTM model. Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1.\n",
    "\n",
    "The problem is really a single character classification problem with 47 classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j92qyDd0m-Wu"
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7TwbcAunOYz"
   },
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence.\n",
    "\n",
    "We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n",
    "\n",
    "The network is slow to train (about 300 seconds per epoch on an Nvidia K520 GPU). Because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to instantiate our generative model in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-UJ85YFnPJG"
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VkUyowm9nSas"
   },
   "source": [
    "We can now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1944
    },
    "colab_type": "code",
    "id": "L6OKHT5-nTBh",
    "outputId": "417dfad7-074b-4962-e500-2c119817459d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 2.8073\n",
      "Epoch 00001: loss improved from inf to 2.80713, saving model to weights-improvement-01-2.8071.hdf5\n",
      "144372/144372 [==============================] - 644s 4ms/step - loss: 2.8071\n",
      "Epoch 2/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 2.4493\n",
      "Epoch 00002: loss improved from 2.80713 to 2.44919, saving model to weights-improvement-02-2.4492.hdf5\n",
      "144372/144372 [==============================] - 644s 4ms/step - loss: 2.4492\n",
      "Epoch 3/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 2.2555\n",
      "Epoch 00003: loss improved from 2.44919 to 2.25546, saving model to weights-improvement-03-2.2555.hdf5\n",
      "144372/144372 [==============================] - 646s 4ms/step - loss: 2.2555\n",
      "Epoch 4/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 2.1219\n",
      "Epoch 00004: loss improved from 2.25546 to 2.12192, saving model to weights-improvement-04-2.1219.hdf5\n",
      "144372/144372 [==============================] - 650s 5ms/step - loss: 2.1219\n",
      "Epoch 5/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 2.0162\n",
      "Epoch 00005: loss improved from 2.12192 to 2.01625, saving model to weights-improvement-05-2.0162.hdf5\n",
      "144372/144372 [==============================] - 652s 5ms/step - loss: 2.0162\n",
      "Epoch 6/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.9358\n",
      "Epoch 00006: loss improved from 2.01625 to 1.93575, saving model to weights-improvement-06-1.9358.hdf5\n",
      "144372/144372 [==============================] - 648s 4ms/step - loss: 1.9358\n",
      "Epoch 7/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.8694\n",
      "Epoch 00007: loss improved from 1.93575 to 1.86926, saving model to weights-improvement-07-1.8693.hdf5\n",
      "144372/144372 [==============================] - 668s 5ms/step - loss: 1.8693\n",
      "Epoch 8/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.8082\n",
      "Epoch 00008: loss improved from 1.86926 to 1.80826, saving model to weights-improvement-08-1.8083.hdf5\n",
      "144372/144372 [==============================] - 665s 5ms/step - loss: 1.8083\n",
      "Epoch 9/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.7622\n",
      "Epoch 00009: loss improved from 1.80826 to 1.76216, saving model to weights-improvement-09-1.7622.hdf5\n",
      "144372/144372 [==============================] - 660s 5ms/step - loss: 1.7622\n",
      "Epoch 10/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.7154\n",
      "Epoch 00010: loss improved from 1.76216 to 1.71534, saving model to weights-improvement-10-1.7153.hdf5\n",
      "144372/144372 [==============================] - 669s 5ms/step - loss: 1.7153\n",
      "Epoch 11/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.6777\n",
      "Epoch 00011: loss improved from 1.71534 to 1.67767, saving model to weights-improvement-11-1.6777.hdf5\n",
      "144372/144372 [==============================] - 665s 5ms/step - loss: 1.6777\n",
      "Epoch 12/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.6448\n",
      "Epoch 00012: loss improved from 1.67767 to 1.64469, saving model to weights-improvement-12-1.6447.hdf5\n",
      "144372/144372 [==============================] - 662s 5ms/step - loss: 1.6447\n",
      "Epoch 13/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.6166\n",
      "Epoch 00013: loss improved from 1.64469 to 1.61656, saving model to weights-improvement-13-1.6166.hdf5\n",
      "144372/144372 [==============================] - 663s 5ms/step - loss: 1.6166\n",
      "Epoch 14/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.5833\n",
      "Epoch 00014: loss improved from 1.61656 to 1.58333, saving model to weights-improvement-14-1.5833.hdf5\n",
      "144372/144372 [==============================] - 660s 5ms/step - loss: 1.5833\n",
      "Epoch 15/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.5578\n",
      "Epoch 00015: loss improved from 1.58333 to 1.55770, saving model to weights-improvement-15-1.5577.hdf5\n",
      "144372/144372 [==============================] - 666s 5ms/step - loss: 1.5577\n",
      "Epoch 16/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.5339\n",
      "Epoch 00016: loss improved from 1.55770 to 1.53396, saving model to weights-improvement-16-1.5340.hdf5\n",
      "144372/144372 [==============================] - 662s 5ms/step - loss: 1.5340\n",
      "Epoch 17/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.5095\n",
      "Epoch 00017: loss improved from 1.53396 to 1.50962, saving model to weights-improvement-17-1.5096.hdf5\n",
      "144372/144372 [==============================] - 662s 5ms/step - loss: 1.5096\n",
      "Epoch 18/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.4890\n",
      "Epoch 00018: loss improved from 1.50962 to 1.48902, saving model to weights-improvement-18-1.4890.hdf5\n",
      "144372/144372 [==============================] - 663s 5ms/step - loss: 1.4890\n",
      "Epoch 19/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.4700\n",
      "Epoch 00019: loss improved from 1.48902 to 1.47010, saving model to weights-improvement-19-1.4701.hdf5\n",
      "144372/144372 [==============================] - 664s 5ms/step - loss: 1.4701\n",
      "Epoch 20/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.4509\n",
      "Epoch 00020: loss improved from 1.47010 to 1.45092, saving model to weights-improvement-20-1.4509.hdf5\n",
      "144372/144372 [==============================] - 666s 5ms/step - loss: 1.4509\n",
      "Epoch 21/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.4337\n",
      "Epoch 00021: loss improved from 1.45092 to 1.43376, saving model to weights-improvement-21-1.4338.hdf5\n",
      "144372/144372 [==============================] - 664s 5ms/step - loss: 1.4338\n",
      "Epoch 22/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.4190\n",
      "Epoch 00022: loss improved from 1.43376 to 1.41898, saving model to weights-improvement-22-1.4190.hdf5\n",
      "144372/144372 [==============================] - 665s 5ms/step - loss: 1.4190\n",
      "Epoch 23/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.4052\n",
      "Epoch 00023: loss improved from 1.41898 to 1.40517, saving model to weights-improvement-23-1.4052.hdf5\n",
      "144372/144372 [==============================] - 665s 5ms/step - loss: 1.4052\n",
      "Epoch 24/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.3885\n",
      "Epoch 00024: loss improved from 1.40517 to 1.38852, saving model to weights-improvement-24-1.3885.hdf5\n",
      "144372/144372 [==============================] - 665s 5ms/step - loss: 1.3885\n",
      "Epoch 25/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.3796\n",
      "Epoch 00025: loss improved from 1.38852 to 1.37962, saving model to weights-improvement-25-1.3796.hdf5\n",
      "144372/144372 [==============================] - 664s 5ms/step - loss: 1.3796\n",
      "Epoch 26/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.3694\n",
      "Epoch 00026: loss improved from 1.37962 to 1.36943, saving model to weights-improvement-26-1.3694.hdf5\n",
      "144372/144372 [==============================] - 665s 5ms/step - loss: 1.3694\n",
      "Epoch 27/50\n",
      "144320/144372 [============================>.] - ETA: 0s - loss: 1.3579\n",
      "Epoch 00027: loss improved from 1.36943 to 1.35786, saving model to weights-improvement-27-1.3579.hdf5\n",
      "144372/144372 [==============================] - 664s 5ms/step - loss: 1.3579\n",
      "Epoch 28/50\n",
      " 12352/144372 [=>............................] - ETA: 10:08 - loss: 1.3062Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9wsCZod0P1F"
   },
   "source": [
    "Generating text using the trained LSTM network is relatively straightforward.\n",
    "\n",
    "Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ayzUk_Ek0T-F"
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-38-1.2788.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dp5HpXN50lXA"
   },
   "source": [
    "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEHnBKDU0n1Q"
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zxipdJOD0sGk"
   },
   "source": [
    "Finally, we need to actually make predictions.\n",
    "\n",
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "pF_XRMSQ0tDz",
    "outputId": "c6c07025-39b5-408d-807d-d65f8ded77b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" and alice thought to herself, ‘i\n",
      "wonder what they  \"\n",
      "would be a little girls of the tor of the sime, which was the dormouse shought there was not eo an o\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "characterPrediction-seq50.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
